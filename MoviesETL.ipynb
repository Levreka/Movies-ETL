{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin importing dependencies, importing data and viewing intial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "from config import db_password\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To make our lives easier, define a variable file_dir for the directory that's holding our data.\n",
    "#get it to work https://stackoverflow.com/questions/55852860/python-syntaxerror-unicode-error-unicodeescape-codec-cant-decode-bytes-in\n",
    "#short answer all your slashes need to be doubled for windows is foward slash\n",
    "#if this is not correct cell 4 wont run dont trust the copy path from right click also\n",
    "# you dont copy the path exactly to the file but to the folder that is holding the file\n",
    "file_dir = \"C:\\\\Users\\\\frank\\\\Desktop\\\\Movies-ETL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f'{file_dir}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, if you want to open a file in your directory, you can use an f-string \n",
    "#(see below) instead of having to type out the whole directory every time. \n",
    "#If you move your files, you only need to update the file_dir variable.\n",
    "#also make sure you are not using a zip file path or we wont be able to get to\n",
    "#data you need to pull out of the zip file the slashes on the path are very important\n",
    "\n",
    "with open(f'{file_dir}\\\\wikipedia-movies.json', mode='r') as file:\n",
    "    wiki_movies_raw = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking the \n",
    "len(wiki_movies_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# First 5 records\n",
    "wiki_movies_raw[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Last 5 records\n",
    "wiki_movies_raw[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some records in the middle\n",
    "wiki_movies_raw[3600:3605]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the csv files downloaded from kaggle.com \n",
    "kaggle_metadata = pd.read_csv(f'movies_metadata.csv', low_memory=False)\n",
    "ratings = pd.read_csv(f'ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viewing a random sample from the data set \n",
    "kaggle_metadata.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial investigation on data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting our wiki movies raw into a pandas dataframe we essentially converting our wiki\n",
    "#json file to a pandas dataframe\n",
    "wiki_movies_df = pd.DataFrame(wiki_movies_raw)\n",
    "#using head method to view the first 5 of the data too many rows to view\n",
    "# there are columns we can view them all even if we do print(wiki_movies_df.columns)\n",
    "# it wont print all of themm to see data sets like this we need to conver them in a list\n",
    "wiki_movies_df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# converting our data frame columns into a list to view them\n",
    "#sorted is optional for easier to read if not needed delete the sort and paranthesis \n",
    "wiki_movies_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use list comprehension to filter data and constrain columns\n",
    "wiki_movies = [movie for movie in wiki_movies_raw\n",
    "               if ('Director' in movie or 'Directed by' in movie)\n",
    "                   and 'imdb_link' in movie\n",
    "                   and 'No. of episodes' not in movie]\n",
    "len (wiki_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a data frame from wiki movies\n",
    "wiki_movies_df= pd.DataFrame(wiki_movies)\n",
    "wiki_movies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #practicing function to clean data not final code \n",
    "# #clean movie is name of the function (movie) is the name of the dic\n",
    "# def clean_movie(movie):\n",
    "#     movie = dict(movie) #create a non-destructive copy this means we will have two copies an orignal and one that the function actually affects\n",
    "    \n",
    "#     #output of our function this is just a skeleton\n",
    "#     return movie\n",
    "\n",
    "#IMPORTANT IF YOU DONT COMMENT THIS OUT CODE LATER WONT WORK BECAUSE YOU ARE SETTING UP TWO FUNCTIONS WITH THE SAME NAME \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at the two arabic language with notnull values in the urls and that have imdb_link\n",
    "wiki_movies_df[wiki_movies_df['Arabic'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are using to see what other columns have other languages so alternative tittles\n",
    "sorted(wiki_movies_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Now we can add in code to handle the alternative titles. The logic we need to implement follows:\n",
    "\n",
    "Make an empty dict to hold all of the alternative titles.\n",
    "\n",
    "Loop through a list of all alternative title keys:\n",
    "\n",
    "Check if the current key exists in the movie object.\n",
    "\n",
    "If so, remove the key-value pair and add to the alternative titles dict.\n",
    "\n",
    "After looping through every key, add the alternative titles dict to the movie object.\n",
    "\n",
    "SKILL DRILL\n",
    "Try to implement the logic above in your clean_movie function on your own.\n",
    "\n",
    "Hint: To remove a key-value pair from a dict in Python, use the pop() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# begin creation of function to clean our movie data/ remeber functions created from \n",
    "#scratch begin wtih the def wording\n",
    "\n",
    "def clean_movie(movie):\n",
    "    movie = dict(movie) #create a non-destructive copy\n",
    "    alt_titles = {}#create an empty dict that holds alternative titles\n",
    "    #looping through a list of all alternative title keys:\n",
    "    for key in ['Also known as','Arabic','Cantonese','Chinese','French',\n",
    "                'Hangul','Hebrew','Hepburn','Japanese','Literally',\n",
    "                'Mandarin','McCuneâ€“Reischauer','Original title','Polish',\n",
    "                'Revised Romanization','Romanized','Russian',\n",
    "                'Simplified','Traditional','Yiddish']:\n",
    "        #setting our conditions using \"if\" the key actually excist in the \n",
    "        #columns when looping it will add the key to the alt_titles empty dict\n",
    "        if key in movie:\n",
    "            alt_titles[key] = movie[key]\n",
    "            #using pop method to remove the key-value pair from the dictionary\n",
    "            movie.pop(key)\n",
    "            \n",
    " # after looping add the alternative titles dcit to the movie object          \n",
    "    if len(alt_titles) > 0:#explaining if the lenght of the alt titles is greater than 0\n",
    "        #create a column in the movie object called alt_titles and have \n",
    "        #the results of the loop added there\n",
    "        movie['alt_titles'] = alt_titles#\n",
    "    #merge column names by creating a new function withing our function\n",
    "    def change_column_name(old_name, new_name):#the Paranthesis is just telling\n",
    "        #our code that old_name will be on the left and new_names on right\n",
    "        \n",
    "        \n",
    "#this code is starting not our loop but our conditions if the old name in the column\n",
    "#is present which in our case it will be since we look at the data before deciding which \n",
    "#columns to change. than change it to the new name which is on the right inside\n",
    "#the paranthesis \n",
    "        if old_name in movie:\n",
    "            movie[new_name] = movie.pop(old_name)#pop the old name so only the new name remains\n",
    "    change_column_name('Adaptation by', 'Writer(s)')\n",
    "    change_column_name('Country of origin', 'Country')\n",
    "    change_column_name('Directed by', 'Director')\n",
    "    change_column_name('Distributed by', 'Distributor')\n",
    "    change_column_name('Edited by', 'Editor(s)')\n",
    "    change_column_name('Length', 'Running time')\n",
    "    change_column_name('Original release', 'Release date')\n",
    "    change_column_name('Music by', 'Composer(s)')\n",
    "    change_column_name('Produced by', 'Producer(s)')\n",
    "    change_column_name('Producer', 'Producer(s)')\n",
    "    change_column_name('Productioncompanies ', 'Production company(s)')\n",
    "    change_column_name('Productioncompany ', 'Production company(s)')\n",
    "    change_column_name('Released', 'Release Date')\n",
    "    change_column_name('Release Date', 'Release date')\n",
    "    change_column_name('Screen story by', 'Writer(s)')\n",
    "    change_column_name('Screenplay by', 'Writer(s)')\n",
    "    change_column_name('Story by', 'Writer(s)')\n",
    "    change_column_name('Theme music composer', 'Composer(s)')\n",
    "    change_column_name('Written by', 'Writer(s)')\n",
    "    return movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can rerun our list comprehension to clean wiki_movies and recreate wiki_movies_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#notice the first line of code is setting a variable to hold our results\n",
    "#in the brackets we are calling the function clean_movie(movie)\n",
    "clean_movies = [clean_movie(movie) for movie in wiki_movies]\n",
    "wiki_movies_df = pd.DataFrame(clean_movies)#recreating the wiki movies df using the clean\n",
    "sorted(wiki_movies_df.columns.tolist())#ordering the list in alphabetical order\n",
    "\n",
    "\n",
    "#NOTICED THIS IS ALMOS THE SAME THING WE DID IN PREVIOUS CELLS BUT NOW WE ARE \n",
    "#CREATING THIS LIST COMPREHENSION WITH THE FUNCTION CREATED STILL GOOD FOR\n",
    "#EXPLORATORY PURPOSES TO BUILD IT LIKE THIS EXPLORE IT FIRST \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll use regular expressions in Pandas' built-in string methods that work on a Series object accessed with the str property. We'll be using str.extract(), which takes in a regular expression pattern. IMDb links generally look like \"https://www.imdb.com/title/tt1234567/,\" with \"tt1234567\" as the IMDb ID. The regular expression for a group of characters that start with \"tt\" and has seven digits is \"(tt\\d{7})\".\n",
    "\n",
    "\"(tt\\d{7})\" â€” The parentheses marks say to look for one group of text.\n",
    "\"(tt\\d{7})\" â€” The \"tt\" in the string simply says to match two lowercase Ts.\n",
    "\"(tt\\d{7})\" â€” The \"\\d\" says to match a numerical digit.\n",
    "\"(tt\\d{7})\" â€” The \"{7}\" says to match the last thing (numerical digits) exactly seven times.\n",
    "\n",
    "Since regular expressions use backslashes, which Python also uses for special characters, we want to tell Python to treat our regular expression characters as a raw string of text. Therefore, we put an r before the quotes. We need to do this every time we create a regular expression string. Weâ€™ll put the extracted IMDB ID into a new column. Altogether, the code to extract the IMDb \n",
    "\n",
    "Now we can drop any duplicates of IMDb IDs by using the drop_duplicates() method. To specify that we only want to consider the IMDb ID, use the subset argument, and set inplace equal to True so that the operation is performed on the selected dataframe. Otherwise, the operation would return an edited dataframe that would need to be saved to a new variable. We also want to see the new number of rows and how many rows were dropped. The whole cell should look like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the cell above explains what is going on with this code to an understandable degree\n",
    "wiki_movies_df['imdb_id'] = wiki_movies_df['imdb_link'].str.extract(r'(tt\\d{7})')\n",
    "print(len(wiki_movies_df))\n",
    "wiki_movies_df.drop_duplicates(subset='imdb_id', inplace=True)\n",
    "print(len(wiki_movies_df))\n",
    "wiki_movies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One way to get the count of null values for each column is to use a list comprehension\n",
    "wiki_columns_to_keep = [column for column in wiki_movies_df.columns if wiki_movies_df[column].isnull().sum() < len(wiki_movies_df) * 0.9]\n",
    "wiki_movies_df = wiki_movies_df[wiki_columns_to_keep]\n",
    "\n",
    "#this code will reduce our data into smaller usefull data to 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_movies_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#looking at the different data types in our columns we are doing this after we got all\n",
    "#of our data clean because we want to make sure that columns with numbers and\n",
    "#strings have the right data types other wise we cant run analysis on them\n",
    "wiki_movies_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#first step to converting box office object into integer data type we drop missing values\n",
    "box_office = wiki_movies_df['Box office'].dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the number of data points thatr exist after dropping the missing values\n",
    "box_office"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expressions only work on strings, so we'll need to make sure all of the box office data is entered as a string. By using the map() method, we can see which values are not strings. First, make a is_not_a_string() function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#althought this code works the function makes it so we would have to name different\n",
    "#functions everytime we run the map() to use it on other columns\n",
    "def is_not_a_string(x):\n",
    "    return type(x) != str\n",
    "\n",
    "box_office[box_office.map(is_not_a_string)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#instead of creating different functions for everys column like the previous\n",
    "#cell we can use lambda function wich is anonymous and we can combine with the map\n",
    "box_office[box_office.map(lambda x: type(x) != str)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rom the output, we can see that there are quite a few data points that are stored as lists. There is a join() string method that concatenates list items into one string; however, we can't just type join(some_list) because the join() method belongs to string objects. We need to make a separator string and then call the join() method on it. For example, the code would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using joint method and using  a simple space as our joining character and \n",
    "#apply the join() function only when our data points are lists \n",
    "box_office = box_office.apply(lambda x: ' '.join(x) if type(x) == list else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_office"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#begining our first form regular expression code visit https://regex101.com/ for playing\n",
    "#and testing some code: this code is really complex look at the 8.3.9-8.3.10 or\n",
    "#go to reular expression actual domcumentation each special symbol has an \n",
    "#specific job that does different things \n",
    "form_one = r\"\\$\\d+\\.?\\d*\\s*[mb]illion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since ther is upper and lower case in our instead of adding it to the code above\n",
    "#we use this method to ignore the case .sum()is optional is just to view how many\n",
    "#points of data were capture\n",
    "box_office.str.contains(form_one, flags=re.IGNORECASE).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the second form that our regular expression will match .sum() is optional\n",
    "#just to view how many points were captured by form two\n",
    "form_two = r'\\$\\d{1,3}(?:,\\d{3})+'\n",
    "box_office.str.contains(form_two, flags=re.IGNORECASE, na=False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the total of when we run our lamba function on previous cell was 5485 currently\n",
    "#with the the form created if we add them the total is 5440 which means there are \n",
    "#some data not being match by our two forms this boolean series is to help us\n",
    "#identify those missing forms this are essentially containers to some degree\n",
    "#holding our data for further analysis \n",
    "matches_form_one = box_office.str.contains(form_one, flags=re.IGNORECASE, na=False)\n",
    "matches_form_two = box_office.str.contains(form_two, flags=re.IGNORECASE, na=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above will give you a ValueError with the explanation \"The truth value of a Series is ambiguous.\" (Unfortunately, the meaning of that error is also ambiguous.)\n",
    "\n",
    "Instead, Pandas has element-wise logical operators:\n",
    "\n",
    "The element-wise negation operator is the tilde: ~ (similar to \"not\")\n",
    "The element-wise logical \"and\" is the ampersand: &\n",
    "The element-wise logical \"or\" is the pipe: |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this will throw an error!\n",
    "# box_office[(not matches_form_one) and (not matches_form_two)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#this code follows the comments on the above cell because pandas uses\n",
    "#element wise operators \n",
    "box_office[~matches_form_one & ~matches_form_two]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Some values have spaces in between the dollar sign and the number.\n",
    "This is easy to fix. Just add \\s* after the dollar signs. The new forms should \n",
    "\n",
    " Some values use a period as a thousands separator, not a comma.\n",
    "This is slightly more complicated, but doable. Simply change form_two to allow for either a comma or period as a thousands separator. Weâ€™d ordinarily do that by putting the comma and period inside straight brackets [,.], but the period needs to be escaped with a slash [,\\.]\n",
    "\n",
    " \"Million\" is sometimes misspelled as \"millon.\"\n",
    "This is easy enough to fix; we can just make the second \"i\" optional in our match string with a question mark as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are editing the code previously created for both forms after we view our results\n",
    "#the above comments are for the following two lines of code \n",
    "form_one = r'\\$\\s*\\d+\\.?\\d*\\s*[mb]illi?on'\n",
    "form_two = r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illion)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Some values are given as a range.\n",
    "To solve this problem, we'll search for any string that starts with a dollar sign and ends with a hyphen, and then replace it with just a dollar sign using the replace() method. The first argument in the replace() method is the substring that will be replaced, and the second argument in the replace() method is the string to replace it with. We can use regular expressions in the first argument by sending the parameter regex=True, as shown below.\n",
    "\n",
    "The rest of the box office values make up such a small percentage of the dataset and would require too much time and effort to parse correctly, so we'll just ignore them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comments above pertain to this code \n",
    "box_office = box_office.str.replace(r'\\$.*[-â€”â€“](?![a-z])', '$', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got expressions to match almost all the box office values, we'll use them to extract only the parts of the strings that match. We do this with the str.extract() method. This method also takes in a regular expression string, but it returns a DataFrame where every column is the data that matches a capture group. We need to make a regular expression that captures data when it matches either form_one or form_two. We can do this easily with an f-string.\n",
    "\n",
    "The f-string f'{form_one}|{form_two}' will create a regular expression that matches either form_one or form_two, so we just need to put the whole thing in parentheses to create a capture group. Our final string will be f'({form_one}|{form_two})', and the full line of code to extract the data follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract and convert the box office values using str.extract for explanation look \n",
    "#at comments above \n",
    "box_office.str.extract(f'({form_one}|{form_two})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need a function to turn the extracted values into a numeric value. We'll call it parse_dollars, and parse_dollars will take in a string and return a floating-point number. We'll start by making a skeleton function with comments explaining each step, and then fill in the steps with actual code.\n",
    "\n",
    "Since we're working directly with strings, we'll use the re module to access the regular expression functions. We'll use re.match(pattern, string) to see if our string matches a pattern. To start, we'll make some small alterations to the forms we defined, splitting the million and billion matches from form one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#refer to comments in above cell for further explanation on the following code:\n",
    "def parse_dollars(s):\n",
    "    # if s is not a string, return NaN\n",
    "    if type(s) != str:\n",
    "        return np.nan\n",
    "\n",
    "    # if input is of the form $###.# million-\n",
    "    #form_one = r'\\$\\s*\\d+\\.?\\d*\\s*[mb]illi?on'notcied that we are not doing mb \n",
    "    #we doing million first-need to test if code will work by copying exactly form 1\n",
    "    if re.match(r'\\$\\s*\\d+\\.?\\d*\\s*milli?on', s, flags=re.IGNORECASE):\n",
    "\n",
    "        # remove dollar sign and \" million\"\n",
    "        s = re.sub('\\$|\\s|[a-zA-Z]','', s)\n",
    "\n",
    "        # convert to float and multiply by a million the double ** means exponent\n",
    "        #meaning we are multiplying the value float by 10 to the 6th power\n",
    "        value = float(s) * 10**6\n",
    "\n",
    "        # return value is returning the value of the previous if\n",
    "        #code if there is one and it was match\n",
    "        return value\n",
    "\n",
    "    # if input is of the form $###.# billion\n",
    "    #form_one = r'\\$\\s*\\d+\\.?\\d*\\s*[mb]illi?on'notice this still formm 1 but\n",
    "    #now we are doing billion\n",
    "    elif re.match(r'\\$\\s*\\d+\\.?\\d*\\s*billi?on', s, flags=re.IGNORECASE):\n",
    "\n",
    "        # remove dollar sign and \" billion\"\n",
    "        s = re.sub('\\$|\\s|[a-zA-Z]','', s)\n",
    "\n",
    "        # convert to float and multiply by a billionthe double ** means exponent\n",
    "        #meaning we are multiplying the value float by 10 to the 9th power\n",
    "        value = float(s) * 10**9\n",
    "\n",
    "        # return value is returning the value of the previous elif\n",
    "        #code if there is one and it was match\n",
    "        return value\n",
    "\n",
    "    # if input is of the form $###,###,###\n",
    "    #form_two = r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illion)' noticed here \n",
    "    #we are doing mb further experimentation to see if code would run just like \n",
    "    #form 1\n",
    "    elif re.match(r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illion)', s, flags=re.IGNORECASE):\n",
    "\n",
    "        # remove dollar sign and commas\n",
    "        s = re.sub('\\$|,','', s)\n",
    "\n",
    "        # convert to float\n",
    "        value = float(s)\n",
    "\n",
    "        # return value is returning the value of the previous elif\n",
    "        #code if there is one and it was match\n",
    "        return value\n",
    "\n",
    "    # otherwise, return NaN if the values do not match to either form 1 or 2\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything we need to parse the box office values to numeric values.\n",
    "\n",
    "First, we need to extract the values from box_office using str.extract. Then we'll apply parse_dollars to the first column in the DataFrame returned by str.extract, which in code looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at comments above we are extracting the converted strings from \"Box 0ffice\"\n",
    "#column and transfering to the new column \"box_office\"\n",
    "wiki_movies_df['box_office'] = box_office.str.extract(f'({form_one}|{form_two})', flags=re.IGNORECASE)[0].apply(parse_dollars)\n",
    "wiki_movies_df['box_office']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Droping the box office column\n",
    "wiki_movies_df.drop('Box office', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verfying \"Box office\" column has been drop and replaced by \"box_office\"\n",
    "wiki_movies_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse budget data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we've already done a lot of the heavy lifting for parsing the budget data when we parsed the box office data. We'll use the same pattern matches and see how many budget values are in a different form. First, we need to preprocess the budget data, just like we did for the box office data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a budget variable and dropna\n",
    "budget = wiki_movies_df['Budget'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert any list into string using lambda\n",
    "budget = budget.map(lambda x: ' '.join(x) if type(x) == list else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any values between a dollar sign and a hyphen\n",
    "#(for budgets given in ranges)\n",
    "budget = budget.str.replace(r'\\$.*[-â€”â€“](?![a-z])', '$', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the same pattern matches that you created to parse the box office data, \n",
    "#and apply them without modifications to the budget data. \n",
    "#Then, look at what's left\n",
    "matches_form_one = budget.str.contains(form_one, flags=re.IGNORECASE, na=False)\n",
    "matches_form_two = budget.str.contains(form_two, flags=re.IGNORECASE, na=False)\n",
    "\n",
    "budget[~matches_form_one & ~matches_form_two]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the citation references meaning the numbers in brackets\n",
    "budget = budget.str.replace(r'\\[\\d+\\]\\s*', '')\n",
    "budget[~matches_form_one & ~matches_form_two]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it worth our time to try and parse what we can out of these remaining 30 budget values, or should we just drop them?\n",
    "\n",
    "This is a judgement call. Here is an argument for dropping the remaining 30 budget values. A handful of them don't even have numeric values, and those that do tend to be in a different currency. Converting currencies can open up a whole can of worms about where to get conversion rates, what rates should be used, rates from what date should be used, etc.\n",
    "\n",
    "There are a handful of values that could be parsed into usable data points without worrying about currency conversion, but we have almost 4,700 other budget values to work with, so even 30 values is less than 1% of the data.\n",
    "\n",
    "Given all the time in the world, maybe it would be worth it to get every last data point into our data, but time is a valuable resource, and putting in the time to convert what we can from these remaining values won't give us enough valuable data to be worth our time.\n",
    "\n",
    "Or as they say, \"The juice isn't worth the squeeze.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the budget converted strings and putting them in a new colum \n",
    "#just like we did previously with \"box office\" since our parse code work very good\n",
    "# we didnt need to recreate a new on with this forms and there fore we didnt\n",
    "#have to recreate a new function either\n",
    "wiki_movies_df['budget'] = budget.str.extract(f'({form_one}|{form_two})', flags=re.IGNORECASE)[0].apply(parse_dollars)\n",
    "wiki_movies_df['budget']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just like before we are dropping the original colum \"Budget\" so we are left\n",
    "#with the converted column budget\n",
    "wiki_movies_df.drop('Budget', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verifying that column was drop succesfully\n",
    "wiki_movies_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Release Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing the release date will follow a similar pattern to parsing box office and budget, but with different forms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a variable that hold the non-null values of release date in the dataframe\n",
    "#notice we are also applying a lambda function the lambda function here is sayin\n",
    "# after droping the null values after apply lamba and if x is in the list convert it \n",
    "#to a string that is the ''=empty string\n",
    "\n",
    "release_date = wiki_movies_df['Release date'].dropna().apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "release_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting our regular expression for the different forms used on our Release Date\n",
    "date_form_one = r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s[123]?\\d,\\s\\d{4}'\n",
    "date_form_two = r'\\d{4}.[01]\\d.[0123]\\d'\n",
    "date_form_three = r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s\\d{4}'\n",
    "date_form_four = r'\\d{4}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the dates \n",
    "release_date.str.extract(f'({date_form_one}|{date_form_two}|{date_form_three}|{date_form_four})', flags=re.IGNORECASE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instead of creating our own function to parse the dates, we'll use the built-in to_datetime() method in Pandas. Since there are different date formats, set the infer_datetime_format option to True. The date formats we've targeted are among those that the to_datetime() function can recognize, which explains the infer_datetime_format=True argument below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comments above pertain to this code\n",
    "wiki_movies_df['release_date'] = pd.to_datetime(release_date.str.extract(f'({date_form_one}|{date_form_two}|{date_form_three}|{date_form_four})')[0], infer_datetime_format=True)\n",
    "wiki_movies_df['release_date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Running Time of the movies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a variable that holds the non-null values of release date in the datafram \n",
    "#this lamba function is working just like the one for date release\n",
    "running_time = wiki_movies_df['Running time'].dropna().apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "running_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using string boundaries to see how many entries of running time \n",
    "#notice this is not the total like show on the code above there 366 entries left\n",
    "running_time.str.contains(r'^\\d*\\s*minutes$', flags=re.IGNORECASE, na=False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viewing the remaining 366 entries\n",
    "running_time[running_time.str.contains(r'^\\d*\\s*minutes$', flags=re.IGNORECASE, na=False) != True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make this more general by only marking the beginning of the string, \n",
    "#and accepting other abbreviations of \"minutes\" by only searching up to the letter \"m.\"\n",
    "#this code is essentially the same as cell 56 but we just making it generic to accetp\n",
    "#more \n",
    "running_time.str.contains(r'^\\d*\\s*m', flags=re.IGNORECASE, na=False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixing code again to be able to pull the remaining 17 \n",
    "running_time[running_time.str.contains(r'^\\d*\\s*m', flags=re.IGNORECASE, na=False) != True]\n",
    "#the return are all those times that are left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is another judgment call. It's only 17 entries out of almost 7,000, so it's highly unlikely that our analysis will be affected by just ignoring these data points. In a time crunch, it would be perfectly acceptable to just move on. However, it's not very difficult to parse these new forms, and we'll have more flexible code if we do. If we decide to do another, larger scrape of Wikipedia data, it's entirely possible that a significant portion have their runtime formatted this way.\n",
    "\n",
    "Even though it's a very small number of entries, it's not too hard to parse, so we'll go ahead and parse those, too.\n",
    "\n",
    "We can match all of the hour + minute patterns with one regular expression pattern. Our pattern follows:\n",
    "\n",
    "Start with one or more digits.\n",
    "Have an optional space after the digit and before the letter \"h.\"\n",
    "Capture all the possible abbreviations of \"hour(s).\" To do this, we'll make every letter in \"hours\" optional except the \"h.\"\n",
    "Have an optional space after the \"hours\" marker.\n",
    "Have an optional number of digits for minutes.\n",
    "As a pattern, this looks like \"\\d+\\s*ho?u?r?s?\\s*\\d*\".\n",
    "\n",
    "With our new pattern, it's time to extract values. We only want to extract digits, and we want to allow for both possible patterns. Therefore, we'll add capture groups around the \\d instances as well as add an alternating character. Our code will look like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at comments below as they pertain to both previous and this code\n",
    "running_time_extract = running_time.str.extract(r'(\\d+)\\s*ho?u?r?s?\\s*(\\d*)|(\\d+)\\s*m')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, this new DataFrame is all strings, we'll need to convert them to numeric values. Because we may have captured empty strings, we'll use the to_numeric() method and set the errors argument to 'coerce'. Coercing the errors will turn the empty strings into Not a Number (NaN), then we can use fillna() to change all the NaNs to zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting a lambda function to perform the task above\n",
    "running_time_extract = running_time_extract.apply(lambda col: pd.to_numeric(col, errors='coerce')).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can apply a function that will convert the hour capture groups \n",
    "#and minute capture groups to minutes if the pure minutes capture\n",
    "#group is zero, and save the output to wiki_movies_df\n",
    "wiki_movies_df['running_time'] = running_time_extract.apply(lambda row: row[0]*60 + row[1] if row[2] == 0 else row[2], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop running time from the dataset \n",
    "wiki_movies_df.drop('Running time', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the Kaggle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first look at the kaggle data it comes csv making sure the data types are correct\n",
    "kaggle_metadata.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking if all adult and video columsn are either true or false\n",
    "kaggle_metadata['adult'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Bad Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the bad data\n",
    "kaggle_metadata[~kaggle_metadata['adult'].isin(['True','False'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we'd want to be able to unscramble the rows and recover the data. But since we don't know what caused the data to be scrambled, it's also possible that even if we got all the data into the right columns, the data would still be corrupt.\n",
    "\n",
    "The biggest concern is that none of the data in these rows looks like an imdb_id. Since that's missing, there's no amount of rearranging that will make these rows into good data. We're just going to have to drop them.\n",
    "\n",
    "In fact, since we probably don't want to include adult movies in the hackathon dataset, we'll only keep rows where adult is False, and then drop the \"adult\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeping rows where the adult colun is false and then drop the adult column\n",
    "kaggle_metadata = kaggle_metadata[kaggle_metadata['adult'] == 'False'].drop('adult',axis='columns')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look ath the values of the video column\n",
    "kaggle_metadata['video'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code creates the Boolean column we want. We just need to \n",
    "#assign it back to video:\n",
    "kaggle_metadata['video'] = kaggle_metadata['video'] == 'True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the numeric columns, we can just use the to_numeric() method from Pandas. We'll make sure the errors= argument is set to 'raise', so we'll know if there's any data that can't be converted to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are converting all this into numberic numbers because they appear as objects\n",
    "kaggle_metadata['budget'] = kaggle_metadata['budget'].astype(int)\n",
    "kaggle_metadata['id'] = pd.to_numeric(kaggle_metadata['id'], errors='raise')\n",
    "kaggle_metadata['popularity'] = pd.to_numeric(kaggle_metadata['popularity'], errors='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert release_date to datetime. Since release_date is in a standard format,\n",
    "#to_datetime()will convert it without any fuss.\n",
    "kaggle_metadata['release_date'] = pd.to_datetime(kaggle_metadata['release_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasonability Checks on Ratings Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we'll take a look at the ratings data. We'll use the info() method on the DataFrame. Since the ratings dataset has so many rows, we need to set the null_counts option to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.info(show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our own analysis, we won't be using the timestamp column; however, we will be storing the rating data as its own table in SQL, so we'll need to convert it to a datetime data type. From the MovieLens documentation, the timestamp is the number of seconds since midnight of January 1, 1970.\n",
    "\n",
    "IMPORTANT\n",
    "Storing time values as a data type is difficult, and there are many, many standards out there for time values. Some store time values as text strings, like the ISO format \"1955-11-05T12:00:00,\" but then calculating the difference between two time values is complicated and computationally expensive. The Unix time standard stores points of time as integers, specifically as the number of seconds that have elapsed since midnight of January 1, 1970. This is known as the Unix epoch. There are other epochs in use, but the Unix epoch is by far the most widespread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify in to_datetime() that the origin is 'unix' and time unit seconds\n",
    "pd.to_datetime(ratings['timestamp'], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign the output of the previous code to the timestamp column\n",
    "ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit='s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll look at the statistics of the actual ratings and see if there are any glaring errors. A quick, easy way to do this is to look at a histogram of the rating distributions, and then use the describe() method to print out some stats on central tendency and spread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a histogram aka(barchart) look at comments above for why we are doing\n",
    "#this step\n",
    "\n",
    "#setting the format in which we want our statistics displayed\n",
    "pd.options.display.float_format = '{:20,.2f}'.format\n",
    "#creating our bargraph\n",
    "ratings['rating'].plot(kind='hist')\n",
    "#will print the statistic summary of ratings\n",
    "ratings['rating'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Wikipedia and Kaggle Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print out list of columns identify which ones are redundatn\n",
    "movies_df = pd.merge(wiki_movies_df, kaggle_metadata, on='imdb_id', suffixes=['_wiki','_kaggle'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking a quick look at some of the titles\n",
    "movies_df[['title_wiki','title_kaggle']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the rows where the titles don't match\n",
    "movies_df[movies_df['title_wiki'] != movies_df['title_kaggle']][['title_wiki','title_kaggle']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show any rows where title_kaggle is empty\n",
    "movies_df[(movies_df['title_kaggle'] == '') | (movies_df['title_kaggle'].isnull())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runtime "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, look at running_time versus runtime. A scatter plot is a great way to give us a sense of how similar the columns are to each other. If the two columns were exactly the same, we'd see a scatter plot of a perfectly straight line. Any wildly different values will show up as dots far from that central line, and if one column is missing data, those values will fall on the x-axis or y-axis.\n",
    "\n",
    "Because we're dealing with merged data, we should expect there to be missing values. Scatter plots won't show null values, so we need to fill them in with zeros when we're making our plots to get the whole picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code will fill in missing values with zero and make the scatter plot\n",
    "movies_df.fillna(0).plot(x='running_time', y='runtime', kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are more data points on the origin of the Y axis than on the origin of the X axis. Since the X axis is Wikipedia and the Y axis is Kaggle, this means there are more missing entries in the Wikipedia data set than in the Kaggle data set. Also, most of the runtimes are pretty close to each other but the Wikipedia data has some outliers, so the Kaggle data is probably a better choice here. However, we can also see from the scatter plot that there are movies where Kaggle has 0 for the runtime but Wikipedia has data, so we'll fill in the gaps with Wikipedia data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make another scatter plot to compare the budget between kaggle and wiki\n",
    "movies_df.fillna(0).plot(x='budget_wiki',y='budget_kaggle', kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Wikipedia data appears to have more outliers compared to the Kaggle data. However, there are quite a few movies with no data in the Kaggle column, while Wikipedia does have budget data. Therefore, we'll fill in the gaps with Wikipedia's data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Box Office "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make another scatter plot to view the kaggle and wiki money in flow\n",
    "movies_df.fillna(0).plot(x='box_office', y='revenue', kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at the same scatter plot but taking out the outlier of 1 billion\n",
    "movies_df.fillna(0)[movies_df['box_office'] < 10**9].plot(x='box_office', y='revenue', kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Release Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For release_date_wiki and release_date_kaggle, we can't directly make a scatter plot, because the scatter plot only works on numeric data. However, there's a tricky workaround that we can use. We'll use the regular line plot (which can plot date data), and change the style to only put dots by adding style='.' to the plot() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#review comments above as to how this code is working \n",
    "movies_df[['release_date_wiki','release_date_kaggle']].plot(x='release_date_wiki', y='release_date_kaggle', style='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should investigate that wild outlier around 2006. We're just going to choose some rough cutoff dates to single out that one movie. We'll look for any movie whose release date according to Wikipedia is after 1996, but whose release date according to Kaggle is before 1965. Here's what your code should look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#review comments above for this code as well\n",
    "movies_df[(movies_df['release_date_wiki'] > '1996-01-01') & (movies_df['release_date_kaggle'] < '1965-01-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the indext of the movie that got merged together which is holiday\n",
    "#and from here to eternity\n",
    "movies_df[(movies_df['release_date_wiki'] > '1996-01-01') & (movies_df['release_date_kaggle'] < '1965-01-01')].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop that row that was merged together\n",
    "movies_df = movies_df.drop(movies_df[(movies_df['release_date_wiki'] > '1996-01-01') & (movies_df['release_date_kaggle'] < '1965-01-01')].index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seeing if there is any null values after droping the merged row wiki\n",
    "movies_df[movies_df['release_date_wiki'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seeing if there is any null values after droping the merged row from kaggle\n",
    "movies_df[movies_df['release_date_kaggle'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Wikipedia data is missing release dates for 11 movies:\n",
    "But the Kaggle data isn't missing any release dates. In this case, we'll just drop the Wikipedia data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the language data, we'll compare the value counts of each. However, consider the following code:\n",
    "\n",
    "This code throws an error because some of the language data points are stored as lists.\n",
    "\n",
    "NOTE\n",
    "We don't need to worry about what hashing is right now, but if you're curious, hashing is a clever computer science trick that can be used to speed up algorithms like getting value counts. Hashing converts values, even arbitrarily long strings, to a limited space of numerical values. We'll talk about hashing more when we get to machine learning, but for now, the important part is that Python creates hash values when new objects are created if they are immutable. Since mutable objects can have their values change after being created, the values might change and not match the hash, so Python just refuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read comments above as to why the error occurs with this code\n",
    "movies_df['Language'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the lists in Language to tuples so that the value_counts() method will work.\n",
    "movies_df['Language'].apply(lambda x: tuple(x) if type(x) == list else x).value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kaggle data doesnt contain the language in list so we can just run value_counts()\n",
    "movies_df['original_language'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a trade-off here between the Wikipedia language data and the Kaggle language data. While the Wikipedia data has more information about multiple languages, the Kaggle data is already in a consistent and usable format. Parsing the Wikipedia data may create too many difficulties to make it worthwhile, though.\n",
    "\n",
    "This is another judgment call; there's no clear-cut answer here. However, for better or for worse, decisions that save time are usually the ones that win, so we'll use the Kaggle data here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Competing data:\n",
    "# Wiki                     Movielens                Resolution\n",
    "#--------------------------------------------------------------------------\n",
    "# title_wiki               title_kaggle             Drop Wikipedia\n",
    "# running_time             runtime                  Keep kaggle; fill in zeros w wiki data\n",
    "# budget_wiki              budget_kaggle            keep kaggle; fill in zeros w wiki data\n",
    "# box_office               revenue                  keep kaggle fill in zeroes w wiki data\n",
    "# release_date_wiki        release_date_kaggle      drop wiki\n",
    "# Language                 original_language        drop wiki\n",
    "# Production company(s)    production_companies     drop wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production Companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#starting off by looking at a small number of samples\n",
    "movies_df[['Production company(s)','production_companies']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kaggle data is much more consistent, and it would be difficult, if not impossible, to translate the Wikipedia data into the same format.\n",
    "\n",
    "We'll drop the Wikipedia data in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first drop the title_wiki, release_date_wiki,\n",
    "#Language, and Production company(s) columns\n",
    "movies_df.drop(columns=['title_wiki','release_date_wiki','Language','Production company(s)'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to save time makign a function that fills in missing data for \n",
    "#column pair and then drops the redundant column.\n",
    "def fill_missing_kaggle_data(df, kaggle_column, wiki_column):\n",
    "    df[kaggle_column] = df.apply(\n",
    "        lambda row: row[wiki_column] if row[kaggle_column] == 0 else row[kaggle_column]\n",
    "        , axis=1)\n",
    "    df.drop(columns=wiki_column, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can run the function for the three column pairs that we decided to fill in zeros.\n",
    "fill_missing_kaggle_data(movies_df, 'runtime', 'running_time')\n",
    "fill_missing_kaggle_data(movies_df, 'budget_kaggle', 'budget_wiki')\n",
    "fill_missing_kaggle_data(movies_df, 'revenue', 'box_office')\n",
    "movies_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we've merged our data and filled in values, it's good to check that there aren't any columns with only one value, since that doesn't really provide any information. Don't forget, we need to convert lists to tuples for value_counts() to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the lists to tupples\n",
    "for col in movies_df.columns:\n",
    "    lists_to_tuples = lambda x: tuple(x) if type(x) == list else x\n",
    "    value_counts = movies_df[col].apply(lists_to_tuples).value_counts(dropna=False)\n",
    "    num_values = len(value_counts)\n",
    "    if num_values == 1:\n",
    "        print(col)\n",
    "        #the code shows us that video only has 1 value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since it's false for every row, we don't need to include this column.\n",
    "movies_df['video'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having similar columns near each other helps people looking through the data get a better sense of what information is available. One way to reorder them would be to consider the columns roughly in groups, like this:\n",
    "\n",
    "Identifying information (IDs, titles, URLs, etc.)\n",
    "Quantitative facts (runtime, budget, revenue, etc.)\n",
    "Qualitative facts (genres, languages, country, etc.)\n",
    "Business data (production companies, distributors, etc.)\n",
    "People (producers, director, cast, writers, etc.)\n",
    "The following code is one way to reorder the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re ordering the collums-read comments above for the logic behind this code\n",
    "movies_df = movies_df.loc[:, ['imdb_id','id','title_kaggle','original_title','tagline','belongs_to_collection','url','imdb_link',\n",
    "                       'runtime','budget_kaggle','revenue','release_date_kaggle','popularity','vote_average','vote_count',\n",
    "                       'genres','original_language','overview','spoken_languages','Country',\n",
    "                       'production_companies','production_countries','Distributor',\n",
    "                       'Producer(s)','Director','Starring','Cinematography','Editor(s)','Writer(s)','Composer(s)','Based on'\n",
    "                      ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did not use .loc to reorder the columns and instead passed a list of column names to the indexing operator (i.e. movies_df = movies_df[[â€˜imdb_idâ€™, â€˜title_kaggleâ€™, â€¦ ]]), you may receive a SettingWithCopyWarning. Don't panic! This isn't an error, so your code will continue to work, but it is a warning that your code may not behave as you expect. In this case, your code will work fine, but for best practices, use .loc instead to avoid this warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming the columns to stay consistent \n",
    "movies_df.rename({'id':'kaggle_id',\n",
    "                  'title_kaggle':'title',\n",
    "                  'url':'wikipedia_url',\n",
    "                  'budget_kaggle':'budget',\n",
    "                  'release_date_kaggle':'release_date',\n",
    "                  'Country':'country',\n",
    "                  'Distributor':'distributor',\n",
    "                  'Producer(s)':'producers',\n",
    "                  'Director':'director',\n",
    "                  'Starring':'starring',\n",
    "                  'Cinematography':'cinematography',\n",
    "                  'Editor(s)':'editors',\n",
    "                  'Writer(s)':'writers',\n",
    "                  'Composer(s)':'composers',\n",
    "                  'Based on':'based_on'\n",
    "                 }, axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform and Merge Rating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use a groupby on the \"movieId\" and \"rating\" columns and take the count for each group.\n",
    "# rating_counts = ratings.groupby(['movieId','rating'], as_index=False).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the \"userId\" column to \"count.\"/ technically you coulve just used \n",
    "#the code above renaming this was arbitrary choice\n",
    "# rating_counts = ratings.groupby(['movieId','rating'], as_index=False).count() \\\n",
    "#                 .rename({'userId':'count'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pivot this data so that movieId is the index, the columns will be all\n",
    "#the rating values, and the rows will be the counts for each rating value.\n",
    "#again this code is just like the one above is just being slowly build \n",
    "#you dont need all those steps above \n",
    "rating_counts = ratings.groupby(['movieId','rating'], as_index=False).count() \\\n",
    "                .rename({'userId':'count'}, axis=1) \\\n",
    "                .pivot(index='movieId',columns='rating', values='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename the columns so they're easier to understand. We'll prepend rating_ \n",
    "#to each column with a list comprehension:\n",
    "rating_counts.columns = ['rating_' + str(col) for col in rating_counts.columns]\n",
    "# rating_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using left join to merge rating_counts into movies_df since we want to keep\n",
    "#everything from movies_df regardless if it has a rating or not \n",
    "movies_with_ratings_df = pd.merge(movies_df, rating_counts, left_on='kaggle_id', right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, because not every movie got a rating for each rating level, there will be missing values instead of zeros. We have to fill those in ourselves, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_with_ratings_df[rating_counts.columns] = movies_with_ratings_df[rating_counts.columns].fillna(0)\n",
    "#viewing data frame to verify excutated correctly\n",
    "movies_with_ratings_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Database Engine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The database engine needs to know how to connect to the database. To do that, we make a connection string. For PostgreSQL, the connection string will look like the following:\n",
    "\n",
    "db_string = f\"postgresql://[user]:[password]@[location]:[port]/[database]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the db_string\n",
    "db_string = f\"postgresql://postgres:{db_password}@127.0.0.1:5432/movie_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(db_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the Movie Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save the movies_df DataFrame to a SQL table, we only have to specify the name of the table and the engine in the to_sql() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the movies_df to our sql data base only the movies df not ratings\n",
    "#once you run this code a second time it  will give you an error \n",
    "#since the table already exist and so you have to add if exists as shown:\n",
    "movies_df.to_sql(name='movies', con=engine,if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the Ratings Data\n",
    "The ratings data is too large to import in one statement, so it has to be divided into \"chunks\" of data. To do so, we'll need to reimport the CSV using the chunksize= parameter in read_csv(). This creates an iterable object, so we can make a for loop and append the chunks of data to the new rows to the target SQL table.\n",
    "\n",
    "##CAUTION\n",
    "\n",
    "The to_sql() method also has a chunksize= parameter, but that won't help us with memory concerns. The chunksize= parameter in to_sql() creates smaller transactions sent to SQL to prevent the SQL instance from getting locked up with a large transaction.\n",
    "\n",
    "This can take quite a long time to run (more than an hour). It's a really good idea to print out some information about how it's running.\n",
    "\n",
    "Let's add functionality to this code to print out:\n",
    "\n",
    "How many rows have been imported\n",
    "How much time has elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a variable for the number of rows imported\n",
    "rows_imported = 0\n",
    "\n",
    "# get the start_time from time.time()\n",
    "start_time = time.time()\n",
    "\n",
    "#for the file direct you need to add the double \\\\ before ratings becuase is\n",
    "#windows computer\n",
    "for data in pd.read_csv(f'{file_dir}\\\\ratings.csv', chunksize=1000000):\n",
    "\n",
    "    # print out the range of rows that are being imported\n",
    "    print(f'importing rows {rows_imported} to {rows_imported + len(data)}...', end='')\n",
    "\n",
    "    data.to_sql(name='ratings', con=engine, if_exists='append')\n",
    "\n",
    "    # increment the number of rows imported by the size of 'data'\n",
    "    rows_imported += len(data)\n",
    "\n",
    "# add elapsed time to final print out\n",
    "    print(f'Done. {time.time() - start_time} total seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
